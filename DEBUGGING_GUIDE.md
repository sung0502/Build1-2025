# TimeBuddy Debugging Guide

## ğŸ” Understanding Bot Flow

### When are specialized bots called?

```
USER INPUT
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ROUTER (brain/router.py)              â”‚
â”‚    â”œâ”€ Keyword matching (confidence calc) â”‚
â”‚    â””â”€ If confidence < 0.7 â†’ LLM call    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
ROUTE DECISION (e.g., "PLAN_CREATE")
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. BOT SELECTION (app.py:392-404)        â”‚
â”‚    â”œâ”€ PLAN_CREATE â†’ CreateBot.run()      â”‚
â”‚    â”œâ”€ PLAN_EDIT â†’ EditBot.run()          â”‚
â”‚    â”œâ”€ PLAN_CHECK â†’ CheckBot.run()        â”‚
â”‚    â””â”€ OTHER â†’ OtherBot.run()             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
BOT PROCESSES REQUEST
    â†“
RETURNS ENVELOPE (with results)
```

---

## ğŸ¤– When is the LLM (Model) Actually Called?

The LLM is called in **specific situations**:

### 1. Router LLM Call (brain/router.py:120-156)
**When**: Only when keyword confidence is < 0.7
**Purpose**: Classify unclear user intent
**Method**: `llm.classify_json()`
**Location**: core/llm.py:71-132

### 2. Bot LLM Calls (varies by bot)
- **CreateBot**: NO LLM CALL (uses rule-based parsing only!)
- **EditBot**: May call LLM for task matching
- **CheckBot**: May call LLM for formatting responses
- **OtherBot**: Calls LLM for general questions

---

## ğŸ“Š How to Check if Model is Being Called

### Method 1: Check Terminal/Console Logs

When you run `streamlit run app.py`, you'll see logs like:

```
============================================================
ğŸ§­ ROUTER.route() called with: 'Add meeting tomorrow at 2pm'
   ğŸ“ Keyword decision: PLAN_CREATE (confidence: 0.89)
   âœ… High confidence - using keyword decision: PLAN_CREATE
   ğŸ¯ FINAL DECISION: PLAN_CREATE (confidence: 0.89)
============================================================
ğŸ¤– Calling bot for stage: PLAN_CREATE
   â†’ CreateBot.run()
   âœ… Bot completed - returned envelope
```

**If LLM is called**, you'll see:
```
============================================================
ğŸ§­ ROUTER.route() called with: 'Show me stuff'
   ğŸ“ Keyword decision: PLAN_CHECK (confidence: 0.40)
   âš ï¸ Low confidence - calling LLM for tie-breaking...
ğŸ” LLM.classify_json() CALLED
   Model: gemini-flash-lite-latest
   Prompt: Classify this user message into ONE of these categories:...
   âœ… Parsed JSON: {'stage': 'PLAN_CHECK', 'confidence': 0.85}
   ğŸ¯ FINAL DECISION: PLAN_CHECK (confidence: 0.85)
============================================================
ğŸ¤– Calling bot for stage: PLAN_CHECK
   â†’ CheckBot.run()
```

---

### Method 2: Add Streamlit Debug Display

Add this to `app.py` (after line 302, before main layout):

```python
# DEBUG: Show routing info
if st.session_state.get('DEBUG_MODE', False):
    with st.expander("ğŸ› Debug Info", expanded=True):
        st.write(f"Last route decision: {st.session_state.get('last_route', 'None')}")
        st.write(f"Awaiting confirmation: {st.session_state.awaiting_confirmation}")
        st.write(f"Chat history length: {len(st.session_state.chat_history)}")
```

Then add this in app.py after route_decision (line 388):

```python
# Store for debugging
st.session_state.last_route = f"{route_decision.stage} (conf: {route_decision.confidence:.2f})"
```

Enable by running:
```python
# In Python console or add to app.py temporarily
st.session_state.DEBUG_MODE = True
```

---

## ğŸ”¬ Testing Different Scenarios

### Scenario 1: High Confidence Keyword (NO LLM call)
**Input**: "Add team meeting tomorrow at 2pm"
**Expected**:
- Router uses keyword matching
- Confidence > 0.7
- CreateBot called directly
- **NO LLM API call**

**Logs**:
```
ğŸ“ Keyword decision: PLAN_CREATE (confidence: 0.89)
âœ… High confidence - using keyword decision: PLAN_CREATE
```

---

### Scenario 2: Low Confidence (LLM call for routing)
**Input**: "What do I have?"
**Expected**:
- Router uses keyword matching
- Confidence < 0.7
- **LLM called for tie-breaking**
- CheckBot called

**Logs**:
```
ğŸ“ Keyword decision: PLAN_CHECK (confidence: 0.40)
âš ï¸ Low confidence - calling LLM for tie-breaking...
ğŸ” LLM.classify_json() CALLED
```

---

### Scenario 3: Bot Calls LLM
**Input**: (varies by bot)
**Expected**:
- Bot needs LLM for processing
- **LLM.generate() called**

**Logs**:
```
ğŸ¤– LLM.generate() CALLED
   Model: gemini-flash-lite-latest
   Prompt: ...
   Response: ...
```

---

## ğŸ› ï¸ Debugging Checklist

When debugging, check:

1. âœ… **Is the router being called?**
   - Look for: `ğŸ§­ ROUTER.route() called`

2. âœ… **What's the confidence level?**
   - Look for: `ğŸ“ Keyword decision: XXX (confidence: 0.XX)`
   - < 0.7 = LLM will be called

3. âœ… **Is LLM being called?**
   - Look for: `ğŸ” LLM.classify_json() CALLED` or `ğŸ¤– LLM.generate() CALLED`

4. âœ… **Which bot is handling the request?**
   - Look for: `â†’ CreateBot.run()` / `â†’ EditBot.run()` / etc.

5. âœ… **Did the bot complete?**
   - Look for: `âœ… Bot completed - returned envelope`

---

## ğŸ› Common Issues

### Issue: No logs appearing
**Solution**: Check your terminal/console where you ran `streamlit run app.py`

### Issue: Model not being called when expected
**Reason**: Keyword confidence is high (> 0.7)
**Solution**: This is normal and saves API costs!

### Issue: Model called too often
**Reason**: Unclear user input leads to low keyword confidence
**Solution**: Check if keywords in router.py:70-97 need updating

---

## ğŸ“ Log Output Examples

### Full Example Log (with LLM call):

```
============================================================
ğŸ§­ ROUTER.route() called with: 'idk what to do'
   ğŸ“ Keyword decision: OTHER (confidence: 0.25)
   âš ï¸ Low confidence - calling LLM for tie-breaking...
ğŸ” LLM.classify_json() CALLED
   Model: gemini-flash-lite-latest
   Prompt: Classify this user message into ONE of these categories:
- PLAN_CREATE: User wants...
   âœ… Parsed JSON: {'stage': 'OTHER', 'confidence': 0.9}
   ğŸ¯ FINAL DECISION: OTHER (confidence: 0.90)
============================================================
ğŸ¤– Calling bot for stage: OTHER
   â†’ OtherBot.run()
ğŸ¤– LLM.generate() CALLED
   Model: gemini-flash-lite-latest
   Prompt: User said: "idk what to do"...
   Response: I can help you...
   âœ… Bot completed - returned envelope
```

---

## ğŸ’¡ Tips

1. **Check logs first** - Terminal output shows everything
2. **Keyword routing is preferred** - Saves API costs, faster
3. **Low confidence is OK** - LLM tie-breaker improves accuracy
4. **CreateBot doesn't call LLM** - Uses rule-based parsing (fast!)

---

## ğŸ”— Code References

- Router: `brain/router.py:38-74`
- Bot selection: `app.py:390-404`
- LLM wrapper: `core/llm.py:31-132`
- CreateBot: `brain/bots/plan_create.py:34-86`

---

Generated: 2025-10-21
